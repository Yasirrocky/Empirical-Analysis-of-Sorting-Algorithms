{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Empirical Analysis of Sorting Algorithms\n",
    "\n",
    "## Objective\n",
    "Analyze the time complexity of sorting algorithms (Bubble Sort, Selection Sort, Insertion Sort, and Merge Sort) by measuring execution time on different input sizes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sorting Algorithm Implementations\n",
    "\n",
    "### 2.1 Bubble Sort\n",
    "Time Complexity: O(n¬≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bubble_sort(arr: List[int]) -> List[int]:\n",
    "    \"\"\"Bubble Sort - O(n¬≤) time complexity\"\"\"\n",
    "    arr = arr.copy()\n",
    "    n = len(arr)\n",
    "    \n",
    "    for i in range(n):\n",
    "        swapped = False\n",
    "        for j in range(0, n - i - 1):\n",
    "            if arr[j] > arr[j + 1]:\n",
    "                arr[j], arr[j + 1] = arr[j + 1], arr[j]\n",
    "                swapped = True\n",
    "        if not swapped:\n",
    "            break\n",
    "    return arr\n",
    "\n",
    "# Test\n",
    "test = [64, 34, 25, 12, 22]\n",
    "print(f\"Original: {test}\")\n",
    "print(f\"Sorted: {bubble_sort(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Selection Sort\n",
    "Time Complexity: O(n¬≤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection_sort(arr: List[int]) -> List[int]:\n",
    "    \"\"\"Selection Sort - O(n¬≤) time complexity\"\"\"\n",
    "    arr = arr.copy()\n",
    "    n = len(arr)\n",
    "    \n",
    "    for i in range(n):\n",
    "        min_idx = i\n",
    "        for j in range(i + 1, n):\n",
    "            if arr[j] < arr[min_idx]:\n",
    "                min_idx = j\n",
    "        arr[i], arr[min_idx] = arr[min_idx], arr[i]\n",
    "    return arr\n",
    "\n",
    "# Test\n",
    "test = [64, 34, 25, 12, 22]\n",
    "print(f\"Original: {test}\")\n",
    "print(f\"Sorted: {selection_sort(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Insertion Sort\n",
    "Time Complexity: O(n¬≤) worst/average, O(n) best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertion_sort(arr: List[int]) -> List[int]:\n",
    "    \"\"\"Insertion Sort - O(n¬≤) worst, O(n) best\"\"\"\n",
    "    arr = arr.copy()\n",
    "    n = len(arr)\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        key = arr[i]\n",
    "        j = i - 1\n",
    "        while j >= 0 and arr[j] > key:\n",
    "            arr[j + 1] = arr[j]\n",
    "            j -= 1\n",
    "        arr[j + 1] = key\n",
    "    return arr\n",
    "\n",
    "# Test\n",
    "test = [64, 34, 25, 12, 22]\n",
    "print(f\"Original: {test}\")\n",
    "print(f\"Sorted: {insertion_sort(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Merge Sort\n",
    "Time Complexity: O(n log n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sort(arr: List[int]) -> List[int]:\n",
    "    \"\"\"Merge Sort - O(n log n) time complexity\"\"\"\n",
    "    arr = arr.copy()\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    \n",
    "    mid = len(arr) // 2\n",
    "    left = merge_sort(arr[:mid])\n",
    "    right = merge_sort(arr[mid:])\n",
    "    return merge(left, right)\n",
    "\n",
    "def merge(left: List[int], right: List[int]) -> List[int]:\n",
    "    \"\"\"Helper to merge two sorted arrays\"\"\"\n",
    "    result = []\n",
    "    i = j = 0\n",
    "    while i < len(left) and j < len(right):\n",
    "        if left[i] <= right[j]:\n",
    "            result.append(left[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            result.append(right[j])\n",
    "            j += 1\n",
    "    result.extend(left[i:])\n",
    "    result.extend(right[j:])\n",
    "    return result\n",
    "\n",
    "# Test\n",
    "test = [64, 34, 25, 12, 22]\n",
    "print(f\"Original: {test}\")\n",
    "print(f\"Sorted: {merge_sort(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Test Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays as per assignment requirements\n",
    "arr1 = list(range(1, 6))      # n=5\n",
    "arr2 = list(range(1, 11))     # n=10\n",
    "arr3 = list(range(1, 51))     # n=50\n",
    "arr4 = list(range(1, 101))    # n=100\n",
    "\n",
    "test_arrays = {\n",
    "    'Arr1 (n=5)': arr1,\n",
    "    'Arr2 (n=10)': arr2,\n",
    "    'Arr3 (n=50)': arr3,\n",
    "    'Arr4 (n=100)': arr4\n",
    "}\n",
    "\n",
    "print(\"Test Arrays:\")\n",
    "for name, arr in test_arrays.items():\n",
    "    print(f\"{name}: Length = {len(arr)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Timing Measurement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(sort_func, arr: List[int], runs: int = 5) -> dict:\n",
    "    \"\"\"Measure execution time with multiple runs for averaging\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(runs):\n",
    "        test_arr = arr.copy()\n",
    "        start_time = time.perf_counter()\n",
    "        sort_func(test_arr)\n",
    "        end_time = time.perf_counter()\n",
    "        # Convert to microseconds\n",
    "        elapsed = (end_time - start_time) * 1_000_000\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    return {\n",
    "        'times': times,\n",
    "        'average': np.mean(times),\n",
    "        'min': np.min(times),\n",
    "        'max': np.max(times),\n",
    "        'std': np.std(times)\n",
    "    }\n",
    "\n",
    "print(\"Timing function ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Collection - Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting_algorithms = {\n",
    "    'Bubble Sort': bubble_sort,\n",
    "    'Selection Sort': selection_sort,\n",
    "    'Insertion Sort': insertion_sort,\n",
    "    'Merge Sort': merge_sort\n",
    "}\n",
    "\n",
    "NUM_RUNS = 5\n",
    "results = {algo: {} for algo in sorting_algorithms.keys()}\n",
    "\n",
    "print(\"Running experiments...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for algo_name, algo_func in sorting_algorithms.items():\n",
    "    print(f\"\\n{algo_name}:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for arr_name, arr in test_arrays.items():\n",
    "        timing_data = measure_time(algo_func, arr, runs=NUM_RUNS)\n",
    "        results[algo_name][arr_name] = timing_data\n",
    "        \n",
    "        print(f\"{arr_name}:\")\n",
    "        print(f\"  Avg: {timing_data['average']:.4f} Œºs\")\n",
    "        print(f\"  Min: {timing_data['min']:.4f} Œºs\")\n",
    "        print(f\"  Max: {timing_data['max']:.4f} Œºs\")\n",
    "        print(f\"  Std: {timing_data['std']:.4f} Œºs\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"Data collection complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = []\n",
    "\n",
    "for algo_name in sorting_algorithms.keys():\n",
    "    for arr_name in test_arrays.keys():\n",
    "        avg_time = results[algo_name][arr_name]['average']\n",
    "        summary_data.append({\n",
    "            'Algorithm': algo_name,\n",
    "            'Array': arr_name,\n",
    "            'Size': len(test_arrays[arr_name]),\n",
    "            'Avg Time (Œºs)': round(avg_time, 4),\n",
    "            'Avg Time (ms)': round(avg_time / 1000, 6)\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "print(\"\\nüìä Summary of Execution Times:\\n\")\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization - Individual Algorithm Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [len(arr) for arr in test_arrays.values()]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Execution Time vs Input Size for Each Sorting Algorithm', \n",
    "             fontsize=16, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (algo_name, algo_func) in enumerate(sorting_algorithms.items()):\n",
    "    times = [results[algo_name][arr_name]['average'] \n",
    "             for arr_name in test_arrays.keys()]\n",
    "    \n",
    "    axes[idx].plot(sizes, times, marker='o', linewidth=2, \n",
    "                   markersize=8, label=algo_name)\n",
    "    axes[idx].set_xlabel('Input Size (n)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Avg Time (Œºs)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'{algo_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    axes[idx].legend()\n",
    "    \n",
    "    # Add value labels\n",
    "    for x, y in zip(sizes, times):\n",
    "        axes[idx].annotate(f'{y:.2f}', (x, y), \n",
    "                          textcoords=\"offset points\", \n",
    "                          xytext=(0,10), ha='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('individual_algorithms.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: individual_algorithms.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization - Comparison Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "markers = ['o', 's', '^', 'D']\n",
    "\n",
    "for idx, (algo_name, algo_func) in enumerate(sorting_algorithms.items()):\n",
    "    times = [results[algo_name][arr_name]['average'] \n",
    "             for arr_name in test_arrays.keys()]\n",
    "    plt.plot(sizes, times, marker=markers[idx], linewidth=2.5, \n",
    "             markersize=10, label=algo_name, color=colors[idx])\n",
    "\n",
    "plt.xlabel('Input Size (n)', fontsize=13, fontweight='bold')\n",
    "plt.ylabel('Average Execution Time (Œºs)', fontsize=13, fontweight='bold')\n",
    "plt.title('Comparison of Sorting Algorithms - Execution Time vs Input Size', \n",
    "          fontsize=15, fontweight='bold', pad=20)\n",
    "plt.legend(fontsize=11, loc='upper left')\n",
    "plt.grid(True, alpha=0.3, linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.savefig('sorting_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: sorting_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization - Bar Chart Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Execution Time Comparison Across Algorithms for Each Input Size', \n",
    "             fontsize=16, fontweight='bold')\n",
    "axes = axes.flatten()\n",
    "colors_bar = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "\n",
    "for idx, arr_name in enumerate(test_arrays.keys()):\n",
    "    algo_names = list(sorting_algorithms.keys())\n",
    "    times = [results[algo][arr_name]['average'] for algo in algo_names]\n",
    "    \n",
    "    bars = axes[idx].bar(algo_names, times, color=colors_bar, \n",
    "                         alpha=0.8, edgecolor='black')\n",
    "    axes[idx].set_ylabel('Avg Time (Œºs)', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'{arr_name}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "    axes[idx].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[idx].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                      f'{height:.2f}', ha='center', va='bottom', \n",
    "                      fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bar_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: bar_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Analysis and Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EMPIRICAL ANALYSIS REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìå Key Findings:\\n\")\n",
    "\n",
    "for arr_name in test_arrays.keys():\n",
    "    times_for_size = {algo: results[algo][arr_name]['average'] \n",
    "                      for algo in sorting_algorithms.keys()}\n",
    "    fastest = min(times_for_size, key=times_for_size.get)\n",
    "    slowest = max(times_for_size, key=times_for_size.get)\n",
    "    \n",
    "    print(f\"{arr_name}:\")\n",
    "    print(f\"  ‚ö° Fastest: {fastest} ({times_for_size[fastest]:.4f} Œºs)\")\n",
    "    print(f\"  üêå Slowest: {slowest} ({times_for_size[slowest]:.4f} Œºs)\")\n",
    "    print(f\"  üìä Speedup: {times_for_size[slowest]/times_for_size[fastest]:.2f}x\\n\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüìä Theoretical vs Empirical Analysis:\\n\")\n",
    "\n",
    "print(\"1Ô∏è‚É£ BUBBLE SORT:\")\n",
    "print(\"   - Theoretical: O(n¬≤)\")\n",
    "print(\"   - Observation: Quadratic growth pattern\")\n",
    "print(\"   - Best for already sorted arrays (early termination)\\n\")\n",
    "\n",
    "print(\"2Ô∏è‚É£ SELECTION SORT:\")\n",
    "print(\"   - Theoretical: O(n¬≤)\")\n",
    "print(\"   - Observation: Consistent quadratic behavior\")\n",
    "print(\"   - Always performs same comparisons\\n\")\n",
    "\n",
    "print(\"3Ô∏è‚É£ INSERTION SORT:\")\n",
    "print(\"   - Theoretical: O(n¬≤) worst, O(n) best\")\n",
    "print(\"   - Observation: Very efficient on sorted data\")\n",
    "print(\"   - Good for small/nearly sorted arrays\\n\")\n",
    "\n",
    "print(\"4Ô∏è‚É£ MERGE SORT:\")\n",
    "print(\"   - Theoretical: O(n log n)\")\n",
    "print(\"   - Observation: Consistent O(n log n) performance\")\n",
    "print(\"   - Most efficient for larger datasets\\n\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüéØ Conclusions:\\n\")\n",
    "print(\"‚úì Sorted data: Insertion Sort performs best (O(n) best case)\")\n",
    "print(\"‚úì Small datasets (n<50): Simple algorithms competitive\")\n",
    "print(\"‚úì Large datasets (n‚â•100): Merge Sort dominates\")\n",
    "print(\"‚úì General purpose: Use O(n log n) algorithms (Merge/Quick/Heap Sort)\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_data = []\n",
    "\n",
    "for algo_name in sorting_algorithms.keys():\n",
    "    for arr_name in test_arrays.keys():\n",
    "        timing_data = results[algo_name][arr_name]\n",
    "        detailed_data.append({\n",
    "            'Algorithm': algo_name,\n",
    "            'Array': arr_name,\n",
    "            'Size': len(test_arrays[arr_name]),\n",
    "            'Average (Œºs)': round(timing_data['average'], 4),\n",
    "            'Min (Œºs)': round(timing_data['min'], 4),\n",
    "            'Max (Œºs)': round(timing_data['max'], 4),\n",
    "            'Std Dev (Œºs)': round(timing_data['std'], 4),\n",
    "            'Average (ms)': round(timing_data['average'] / 1000, 6)\n",
    "        })\n",
    "\n",
    "df_detailed = pd.DataFrame(detailed_data)\n",
    "df_detailed.to_csv('sorting_results.csv', index=False)\n",
    "print(\"‚úÖ Results exported to 'sorting_results.csv'\")\n",
    "print(\"\\nPreview:\")\n",
    "print(df_detailed.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
